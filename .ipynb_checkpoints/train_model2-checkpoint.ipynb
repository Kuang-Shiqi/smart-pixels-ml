{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d525f-2bae-44e3-833f-376d07f96cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python based\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom \n",
    "from loss import *\n",
    "from models import *\n",
    "from dataloaders import utils\n",
    "from dataloaders import OptimizedDataGenerator as DG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b48734-3871-4cf4-a78f-d5d973cff193",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num CPU:\", os.cpu_count())\n",
    "print(utils.check_GPU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6512e-f169-4c9c-b327-e0428312a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = '/depot/cms/users/dkondra/smart-pixels/dataset8/unflipped-positive'\n",
    "dataset_path = '/depot/cms/users/das214/dataset8/unflipped'\n",
    "data_directory_path = os.path.join(dataset_path, 'recon3D/')\n",
    "labels_directory_path = os.path.join(dataset_path, 'labels/')\n",
    "\n",
    "data_files_path_list = [os.path.join(data_directory_path, f) for f in os.listdir(data_directory_path)]\n",
    "labels_files_path_list = [os.path.join(labels_directory_path, f) for f in os.listdir(labels_directory_path)]\n",
    "\n",
    "data_files_path_list = np.sort(data_files_path_list)\n",
    "labels_files_path_list = np.sort(labels_files_path_list)\n",
    "\n",
    "print(data_directory_path)\n",
    "print(labels_directory_path)\n",
    "print(len(data_files_path_list))\n",
    "print(len(labels_files_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e495d-c5f6-4423-813e-6971e12da9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = Path(\"./\").resolve()\n",
    "\n",
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = 142\n",
    "val_file_size = 6\n",
    "\n",
    "# batch_size = 500\n",
    "# val_batch_size = 500\n",
    "# train_file_size = 20 \n",
    "# val_file_size = 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9a559-1759-4d4f-b3be-27ccce4d7aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_directory, exist_ok=True)\n",
    "print(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e073a9-dea8-4522-aaff-b06aecf74552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf records directory (random)\n",
    "stamp = '%08x' % random.randrange(16**8)\n",
    "stamp = 1\n",
    "tfrecords_dir_train = Path(output_directory, f\"tfrecords_train_{stamp}\").resolve()\n",
    "tfrecords_dir_validation = Path(output_directory, f\"tfrecords_validation_{stamp}\").resolve()\n",
    "\n",
    "# Path where the TFRecord files will be saved (deterministic)\n",
    "tfrecords_dir_train = \"/depot/cms/users/das214/tfrecords_20t_train_d8\"\n",
    "tfrecords_dir_validation = \"/depot/cms/users/das214/tfrecords_20t_val_d8\"\n",
    "\n",
    "# clean up tf records\n",
    "# utils.safe_remove_directory(tfrecords_dir_train)\n",
    "# utils.safe_remove_directory(tfrecords_dir_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877986e5-4d02-48b9-8ebf-4de28c98420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation generator\n",
    "\n",
    "# Caution: If you want to load older TFRecord files dont run like this instead use `load_from_tfrecords_dir`\n",
    "#       Or else if there exist and data at `tfrecords_dir` will be removed.\n",
    "\n",
    "start_time = time.time()\n",
    "validation_generator = DG.OptimizedDataGenerator(\n",
    "    data_directory_path = data_directory_path,\n",
    "    labels_directory_path = labels_directory_path,\n",
    "    is_directory_recursive = False,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False, \n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, \n",
    "    files_from_end=True,\n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_validation,\n",
    "    use_time_stamps = [0, 19], #-1\n",
    "    max_workers = 2 # Don't make this too large (will use up all RAM)\n",
    ")\n",
    "\n",
    "print(\"--- Validation generator %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0d6b9-6c0a-47f2-a0ab-b84fcc9093b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training generator\n",
    "\n",
    "# Caution: If you want to load older TFRecord files dont run like this instead use `load_from_tfrecords_dir`\n",
    "#       Or else if there exist and data at `tfrecords_dir` will be removed.\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "training_generator = DG.OptimizedDataGenerator(\n",
    "    data_directory_path = data_directory_path,\n",
    "    labels_directory_path = labels_directory_path,\n",
    "    is_directory_recursive = False,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, # True \n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = [0, 19], #-1\n",
    "    max_workers = 2 # Don't make this too large (will use up all RAM)\n",
    ")\n",
    "print(\"--- Training generator %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320dddd-3da1-4624-b681-6daf95fb099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be commented out entirely\n",
    "# This cell shows the implementation of how to load TFRecord files if they are already initialized earlier\n",
    "# Letting the user load from older files saving time (from preprocessing and saving)\n",
    "\n",
    "training_generator = DG.OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train,\n",
    "    shuffle = True,\n",
    "    seed = 13,\n",
    "    quantize = True\n",
    ")\n",
    "\n",
    "validation_generator = DG.OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_validation, \n",
    "    shuffle = True,\n",
    "    seed = 13,\n",
    "    quantize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbc512-2105-4409-9772-f9eb6ab364fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (13, 21, 2)\n",
    "model = CreateModel(input_shape, n_filters=5, pool_size=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99610e5c-7380-4a82-ab1e-b61521ad0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_steps = 90*training_generator.__len__()\n",
    "alpha = 0.01\n",
    "initial_learning_rate = 1e-3\n",
    "warmup_target = 1e-1\n",
    "warmup_steps = 10*training_generator.__len__()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    alpha=alpha,\n",
    "    warmup_target = warmup_target,\n",
    "    warmup_steps = warmup_steps\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Nadam(learning_rate=lr_schedule),\n",
    "#     loss=custom_loss\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f316a1-ea46-47b7-9367-1018c898e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "base_dir = f'./trained_models/model-{fingerprint}-checkpoints'\n",
    "os.makedirs(base_dir, exist_ok=True)  \n",
    "checkpoint_filepath = base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d78bd7-83af-4434-8e82-7a55067426b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5e964-602b-432b-a735-50c38b02e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "early_stopping_patience = 50\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        checkpoints = [f for f in os.listdir(base_dir) if f.startswith('weights')]\n",
    "        if len(checkpoints) > 1:\n",
    "            checkpoints.sort()\n",
    "            for checkpoint in checkpoints[:-1]:\n",
    "                os.remove(os.path.join(base_dir, checkpoint))\n",
    "\n",
    "es = EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "mcp = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(f'{base_dir}/training_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24061ace-13f8-456b-a33c-b80620231e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[es, mcp, csv_logger],\n",
    "    epochs=1000,\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d349fbe3-9e51-436c-994f-6c6eff29f2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02428409-9336-432e-949f-7d0540bbb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean up tf records\n",
    "# utils.safe_remove_directory(tfrecords_dir_train)\n",
    "# utils.safe_remove_directory(tfrecords_dir_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
